{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZL9mrLkc3Dp"
      },
      "outputs": [],
      "source": [
        "!pip install scrapy\n",
        "!pip install spider3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nah1kqOV74Wk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import scrapy\n",
        "from pandas.testing import assert_frame_equal # to compare two dataframes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CADRNR64WnmB"
      },
      "source": [
        "Như bạn đã biết, World Cup là một giải bóng đá lớn nhất thế giới được tổ chức mỗi 4 năm 1 lần. Vì cuối tháng 11 này kỳ World Cup 2022 sẽ diễn ra, do đó ở Lab 1 này, chúng ta sẽ khởi động môn học bằng việc thu thập dữ liệu của các cầu thủ bóng đá. <b>SoFIFA</b> (https://sofifa.com/) là một trang web lưu trữ dữ liệu của các cầu thủ trong trò chơi bóng đá nổi tiếng FIFA 23 mà có các chỉ số phản ánh gần đúng với phong độ của các cầu thủ bóng đá ngoài đời. Trong phần này, nhiệm vụ đầu tiên của bạn là sẽ cần thu thập ID của các cầu thủ. Mình có check \"Terms of Service\" của SoFIFA thì không thấy có mục nào nói là cấm parse HTML, vì vậy với mục đích học thì chắc là không có vấn đề gì lớn, miễn là chương trình của bạn đừng \"hit\" trang web quá nhiều lần trong một khoảng thời gian ngắn thì sẽ không có vấn đề gì. \n",
        "</br></br>\n",
        "Công việc cụ thể của bạn ở phần đầu tiên này là sẽ cần viết class <b>collect_player_url()</b> ở bên dưới. Vì dữ liệu lúc sau các bạn cần thu thập sẽ rất lớn nên ở đây, chúng ta sẽ xài một thư viên có tên scrapy để có thể thu thập dữ liệu lớn được nhanh chóng hơn. Về cách sử dụng thư viện này thì bạn có thể tham khảo thêm tại trang web sau: https://docs.scrapy.org/en/latest/intro/tutorial.html.\n",
        "</br></br>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2><b>Tạo một project mới với scrapy</b><h2>\n",
        "\n",
        "Để sử dụng thư viện scrapy sau khi cài đặt xong, các bạn sẽ gọi câu lệnh như bên dưới để bắt đầu tạo một project mới với scrapy với tên gọi là <b>\"fifa_crawler\"</b>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ndHFd1lWnUj"
      },
      "outputs": [],
      "source": [
        "!scrapy startproject fifa_crawler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyLNgVqWvt3Z"
      },
      "outputs": [],
      "source": [
        "cd fifa_crawler/fifa_crawler"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LtjRbH9NmmI8"
      },
      "source": [
        "Sau khi tạo xong project với scrapy, vì thu thập dữ liệu với thư viện này không cho phép xài notebook trực tiếp nên các bạn sau khi hoàn thành xong class <b>collect_player_url(scrapy.Spider)</b> ở bên dưới, các bạn cần tạo một file có tên <b>collect_players_urls.py</b> vào đường dẫn sau <b>fifa_crawler/fifa_crawler/spiders/collect_players_urls.py</b>. Ngoài ra, để tránh việc <b>hit</b> trang web quá nhiều lần, thay vì thu thập toàn bộ ID các cầu thủ của thì các bạn sẽ chỉ cần thu thập 720 ID cầu thủ. Để cho dễ dàng, mình đã để sẵn 1 đường dẫn urls chứa định dạng offset (từng trang của các cầu thủ với mỗi trang chứa 60 cầu thủ khác nhau) để các bạn có thể dễ dàng chuyển trang mới trong lúc thu thập."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsIYPdgWeJ0z"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "\n",
        "class collect_player_url(scrapy.Spider):\n",
        "    name = 'players_urls'\n",
        "    def __init__(self):\n",
        "        self.count = 60\n",
        "    def start_requests(self):\n",
        "        urls = ['https://sofifa.com/players?col=oa&sort=desc&offset=0']\n",
        "        # # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        for url in urls:\n",
        "            yield scrapy.Request(url=url, callback=self.parse)\n",
        "\n",
        "    def parse(self, response):\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        players_url = response.css('tbody > tr')\n",
        "        for player in players_url:\n",
        "            id = player.css('td.col-name > a::attr(href)').get().split('/')\n",
        "            items = {\n",
        "                \"player_url\": '/' + id[1] + '/' + id[2]\n",
        "            }\n",
        "            yield items\n",
        "\n",
        "        if self.count < 720:\n",
        "            next_page = 'https://sofifa.com/players?col=oa&sort=desc&offset=' + str(self.count)\n",
        "            self.count += 60\n",
        "            yield response.follow(next_page, callback=self.parse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ShUFd8SmfCgf"
      },
      "source": [
        "Sau khi đã viết xong class trên, và đã tạo file <b>collect_players_urls.py</b> trong đường dẫn đã đề cập, bạn sẽ tiến hành gọi lệnh như bên dưới để thu thập ID của các cầu thủ bóng đá và lưu vào một file có tên <b>players_urls.json</b> mà chứa trong thư mục dataset. Thư mục dataset này bạn không cần tạo mà khi thu thập dữ liệu scrapy sẽ tự động tạo cho bạn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLoJ44s6e9NG"
      },
      "outputs": [],
      "source": [
        "!scrapy crawl players_urls -o dataset/players_urls.json"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "caESkJGPopG9"
      },
      "source": [
        "Sau khi đã có danh sách 720 ID của các cầu thủ đã thu thập từ trang web SoFIFA, bạn sẽ tiến hành thu thập dữ liệu cụ thể của từng cầu thủ ứng với các ID này bằng cách hoàn thành class <b>collect_player_info(scrapy.Spider)</b> như bên dưới. Các bạn cũng lưu ý lại là như mình đã đề cập, việc sử dụng scrapy trực tiếp notebook là không được mà chúng ta sẽ cần tạo tiếp một file mới có tên <b>collect_players_info.py</b> vào cùng đường dẫn như file <b>collect_players_urls.py</b>. Ở đây, cũng để cho tiện thì mình cũng để cho các bạn 1 đường dẫn url mới với ID 231747 ứng với ID đầu tiên mà chúng ta đã thu thập ở bước trước. Nhiệm vụ của các bạn là dựa vào url này để tiếp tục hoàn thành việc parse HTML và thu thập các thông tin chi tiết của ID này (chúng ta cũng làm tương tự cho 719 ID còn lại). Về chi tiết tất cả các dữ liệu các bạn cần thu thập thì mình có để sẵn 1 file test có tên <b>players_info.json</b> là file đã được mình thu thập về thông tin chi tiết của 720 cầu thủ cho các bạn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHicINKmvDvC",
        "outputId": "e6693593-7eb2-44e4-d8e5-d38b0993ba83"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_json('./players_info.json', encoding='utf-8-sig')\n",
        "df_test.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vHUkFitpbpv"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "import json\n",
        "\n",
        "\n",
        "class collect_player_info(scrapy.Spider):\n",
        "    name = 'players_info'\n",
        "\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            with open('dataset/players_urls.json') as f:\n",
        "                self.players = json.load(f)\n",
        "            self.player_count = 1\n",
        "        except IOError:\n",
        "            print(\"File not found\")\n",
        "\n",
        "    def start_requests(self):\n",
        "        urls = ['https://sofifa.com/player/231747?units=mks']\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        for url in urls:\n",
        "            yield scrapy.Request(url=url, callback=self.parse)\n",
        "\n",
        "    def parse(self, response):\n",
        "        # YOUR CODE HERE\n",
        "        #   raise NotImplementedError()\n",
        "        players_info = response.css('body > div.center > .grid > .col.col-12')\n",
        "\n",
        "        # hàng tuổi, ngày sinh,...\n",
        "        info = players_info[0].css(\n",
        "            'div.bp3-card > .info > div.meta.ellipsis::text')[-1]\n",
        "        index_overall = players_info[0].css(\n",
        "            'div.bp3-card > .card.spacing > div.block-quarter')\n",
        "        # profile\n",
        "        profile = players_info[0].css('div.block-quarter')[4]\n",
        "        specialities = players_info[0].css('div.block-quarter')[5]\n",
        "        # teams\n",
        "        teams = players_info[0].css('div.block-quarter')[6:8]\n",
        "\n",
        "        # chỉ số cụ thể\n",
        "        primary_position = response.css(\n",
        "            'body > div.center > .grid > .col.col-4')\n",
        "        specific_index = players_info[1].css(\n",
        "            'div.block-quarter')  # từng mục chỉ số\n",
        "        name_specific_index = []  # tên thông số cụ thể\n",
        "        index_specific_index = []  # chỉ số thông số cụ thể\n",
        "        for i in range(len(specific_index)):\n",
        "            duplicate_index = specific_index[i].css(\n",
        "                'div.card > ul.pl > li > span.plus::text').getall()\n",
        "            duplicate_index += specific_index[i].css(\n",
        "                'div.card > ul.pl > li > span.minus::text').getall()\n",
        "            values = specific_index[i].css(\n",
        "                'div.card > ul.pl > li > span::text').getall()\n",
        "            values = [i for i in values if i not in duplicate_index] #xóa phần tử chỉ số plus\n",
        "            \n",
        "            name_specific_index.append(values[1::2])\n",
        "            name_specific_index[i] = [\n",
        "                x.replace(\" \", \"\") for x in name_specific_index[i]]\n",
        "            index_specific_index.append(values[::2])\n",
        "\n",
        "        items = {\n",
        "            \"id\": profile.css('div.card > ul.pl >.ellipsis::text')[-1].get(),\n",
        "            \"name\": players_info[0].css('.info > h1::text').get(),\n",
        "            \"primary_position\": primary_position[1].css('ul.pl > li.ellipsis > .pos::text').get(),\n",
        "            \"positions\": players_info[0].css('.info > div.meta.ellipsis > span.pos::text').getall(),\n",
        "            \"age\": info.get().split(' ')[1][:-4],\n",
        "            \"birth_date\": info.get().split(' ')[4][:-1] + '/'+info.get().split(' ')[2][1:] + '/'+info.get().split(' ')[3][:-1],\n",
        "            \"height\": int(info.get().split(' ')[5][:-2]),\n",
        "            \"weight\": int(info.get().split(' ')[6][:-2]),\n",
        "            \"Overall Rating\": int(index_overall.css('span.bp3-tag::text')[0].get()),\n",
        "            \"Potential\": int(index_overall.css('span.bp3-tag::text')[1].get()),\n",
        "            \"Value\": index_overall.css('div::text')[4].get(),\n",
        "            \"Wage\": index_overall.css('div::text')[6].get(),\n",
        "        }\n",
        "        #xử lý mục profile\n",
        "        profile_keys = profile.css(\n",
        "            'div.card > ul.pl >.ellipsis > label::text').getall()[:-1]\n",
        "        upper_profile_value = [i for i in profile.css('div.card > ul.pl >.ellipsis::text').getall(\n",
        "        ) if i != ' '][:-1]\n",
        "        index_of_upper_profile_value = list(map(int,upper_profile_value[1:]))\n",
        "        index_of_upper_profile_value.insert(0,upper_profile_value[0])\n",
        "\n",
        "        profile_values = index_of_upper_profile_value + profile.css('div.card > ul.pl >.ellipsis > span::text').getall()[-4:]\n",
        "        \n",
        "        for i in range(len(profile_keys)):\n",
        "            items[profile_keys[i]] = profile_values[i]\n",
        "            \n",
        "        # Xử lý mục teams\n",
        "        u = {}\n",
        "        for i in range(len(teams)):\n",
        "            u[teams.css('div.card > h5 > a::text').getall()[i]] = int(\n",
        "                teams.css('div.card > ul.ellipsis.pl > li > span.bp3-tag::text')[i].get())\n",
        "        items[\"teams\"] = u\n",
        "        \n",
        "        x = {\n",
        "            specific_index[0].css('div.card>h5::text').get().lower(): {\n",
        "                name_specific_index[0][0]: int(index_specific_index[0][0]),\n",
        "                name_specific_index[0][1]: int(index_specific_index[0][1]),\n",
        "                name_specific_index[0][2]: int(index_specific_index[0][2]),\n",
        "                name_specific_index[0][3]: int(index_specific_index[0][3]),\n",
        "                name_specific_index[0][4]: int(index_specific_index[0][4]),\n",
        "            },\n",
        "            specific_index[1].css('div.card>h5::text').get().lower(): {\n",
        "                name_specific_index[1][0]: int(index_specific_index[1][0]),\n",
        "                name_specific_index[1][1]: int(index_specific_index[1][1]),\n",
        "                name_specific_index[1][2]: int(index_specific_index[1][2]),\n",
        "                name_specific_index[1][3]: int(index_specific_index[1][3]),\n",
        "                name_specific_index[1][4]: int(index_specific_index[1][4]),\n",
        "            },\n",
        "            specific_index[2].css('div.card>h5::text').get().lower(): {\n",
        "                name_specific_index[2][0]: int(index_specific_index[2][0]),\n",
        "                name_specific_index[2][1]: int(index_specific_index[2][1]),\n",
        "                name_specific_index[2][2]: int(index_specific_index[2][2]),\n",
        "                name_specific_index[2][3]: int(index_specific_index[2][3]),\n",
        "                name_specific_index[2][4]: int(index_specific_index[2][4]),\n",
        "            },\n",
        "            specific_index[3].css('div.card>h5::text').get().lower(): {\n",
        "                name_specific_index[3][0]: int(index_specific_index[3][0]),\n",
        "                name_specific_index[3][1]: int(index_specific_index[3][1]),\n",
        "                name_specific_index[3][2]: int(index_specific_index[3][2]),\n",
        "                name_specific_index[3][3]: int(index_specific_index[3][3]),\n",
        "                name_specific_index[3][4]: int(index_specific_index[3][4]),\n",
        "            },\n",
        "            specific_index[4].css('div.card>h5::text').get().lower(): {\n",
        "                name_specific_index[4][0]: int(index_specific_index[4][0]),\n",
        "                name_specific_index[4][1]: int(index_specific_index[4][1]),\n",
        "                name_specific_index[4][2]: int(index_specific_index[4][2]),\n",
        "                name_specific_index[4][3]: int(index_specific_index[4][3]),\n",
        "                name_specific_index[4][4]: int(index_specific_index[4][4]),\n",
        "                name_specific_index[4][5]: int(index_specific_index[4][5]),\n",
        "            },\n",
        "            specific_index[5].css('div.card>h5::text').get().lower(): {\n",
        "                name_specific_index[5][0]: int(index_specific_index[5][0]),\n",
        "                name_specific_index[5][1]: int(index_specific_index[5][1]),\n",
        "                name_specific_index[5][2]: int(index_specific_index[5][2]),\n",
        "            },\n",
        "            specific_index[6].css('div.card>h5::text').get().lower(): {\n",
        "                name_specific_index[6][0]: int(index_specific_index[6][0]),\n",
        "                name_specific_index[6][1]: int(index_specific_index[6][1]),\n",
        "                name_specific_index[6][2]: int(index_specific_index[6][2]),\n",
        "                name_specific_index[6][3]: int(index_specific_index[6][3]),\n",
        "                name_specific_index[6][4]: int(index_specific_index[6][4]),\n",
        "            },\n",
        "            \"player_traits\":\n",
        "                specific_index[7].css(\n",
        "                    'div.card > ul.pl > li > span::text').getall(),\n",
        "            \"player_specialities\": specialities.css('div.card > ul.pl >.ellipsis > a::text').getall()\n",
        "        }\n",
        "        items.update(x)\n",
        "        yield items\n",
        "\n",
        "        if self.player_count < len(self.players):\n",
        "            next_page_url = 'https://sofifa.com' + \\\n",
        "                self.players[self.player_count]['player_url'] + '?units=mks'\n",
        "            self.player_count += 1\n",
        "            yield scrapy.Request(url=next_page_url, callback=self.parse)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WMtk8cmuqnXm"
      },
      "source": [
        "Sau khi đã hoàn thành class ở trên và lưu lại trong file <b>collect_players_info.py</b>, các bạn sẽ tiếp tục chạy câu lệnh bên dưới để thu thập thông tin chi tiết của toàn bộ 720 cầu thủ và xuất ra file <b>players_info.json</b> ở cùng đường dẫn dataset như trên."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeWsqMVMpNpW"
      },
      "outputs": [],
      "source": [
        "!scrapy crawl players_info -o dataset/players_info.json"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "swqaBClCrg39"
      },
      "source": [
        "Sau khi đã có thông tin chi tiết của 720 cầu thủ, các bạn sẽ tiến hành đọc file <b>players_info.json</b> vào pandas với tên gọi <b>df_players_info</b> và kiểm tra lại với file của mình xem dữ liệu các bạn thu thập là khớp chưa. Nếu đã trùng khớp hai file thì bạn sẽ đến với bước tiền xử lý dữ liệu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIzyLHd2rPwI"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# raise NotImplementedError()\n",
        "df_players_info = pd.read_json('./fifa_crawler/fifa_crawler/dataset/players_info.json', encoding='utf-8-sig')\n",
        "assert_frame_equal(df_players_info, df_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "crik-hTWt1RH"
      },
      "source": [
        "<h2><b>Tiền xử lý dữ liệu</b></h2>\n",
        "\n",
        "<b>1)</b> Sau khi đã kiểm tra và khớp với dữ liệu từ file mình đã cung cấp, nhiệm vụ đầu tiên của các bạn trong bước tiền xử lý dữ liệu là cần phân tách các chỉ số cụ thể của mỗi cầu thủ. Như chúng ta thấy, ở mỗi cột <b>attacking, skill, movement</b> sẽ là một từ điển chứa chi tiết cụ thể từng chỉ số chi tiết ở trong đó. Do đó, chúng ta sẽ cần phân tách các từ điển này thành các cột cụ thể và nhiệm vụ khi này của bạn là hoàn thành hàm <b>split_players_info()</b> ở bên dưới rồi lưu lại vào một dataframe mới với tên <b>df_split_players_info</b>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFCvJ3r51ZHb"
      },
      "outputs": [],
      "source": [
        "def split_players_info():\n",
        "    # YOUR CODE HERE\n",
        "    # raise NotImplementedError()\n",
        "    # lấy tên cột attacking, skill,...\n",
        "    names_skill = df_players_info.columns[-9:-2]\n",
        "\n",
        "    skill_keys = []  # lưu tên các thuộc tính của mỗi cột\n",
        "    for i in names_skill:\n",
        "        skill_keys += df_players_info[i][0].keys()\n",
        "\n",
        "    skill_dict = {}\n",
        "    for index in skill_keys:\n",
        "        skill_dict[index] = {}\n",
        "\n",
        "    for i in range(len(df_players_info)):\n",
        "        for element in names_skill:\n",
        "            for index in skill_keys:\n",
        "                if index in df_players_info[element][0].keys():\n",
        "                    skill_dict[index][i] = df_players_info[element][i][index]\n",
        "\n",
        "    df_split_players_info = pd.DataFrame(skill_dict)\n",
        "    df_split_players_info = pd.concat([df_players_info, df_split_players_info], axis=1)\n",
        "    for i in names_skill:\n",
        "        del df_split_players_info[i]\n",
        "\n",
        "    return df_split_players_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trd9o5bUEujU"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "df_split_players_info = split_players_info()\n",
        "df_split_players_info_test = pd.read_json('./split_players_info.json', encoding='utf-8-sig')\n",
        "assert_frame_equal(df_split_players_info.iloc[:, 23:], df_split_players_info_test.iloc[:, 23:])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VQtEUA263lFE"
      },
      "source": [
        "<b>2)</b> Vì cột giá trị của cầu thủ (Value) và tiền lương (Wage) đang ở dạng viết tắt nên các bạn cần chuyển hai cột này về dạng số (float) bằng cách hoàn thành hàm <b>value_and_wage_to_float(col)</b>. Ví dụ, cầu thủ A có giá trị là €1M và tiền lương là €1K, hai cột này sau khi được tiền xử lý thì cầu thủ A sẽ có giá trị là 1000000.0 và tiền lương là 1000. Ngoài ra, bạn cần lưu ý kiểm tra các cột này có giá trị bị thiếu hay không, nếu có sẽ điền là 0.0 và ngược lại sẽ tiền xử lý như trên."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w1ySIhU3J_O"
      },
      "outputs": [],
      "source": [
        "def value_and_wage_to_float(col):\n",
        "    # YOUR CODE HERE\n",
        "    # raise NotImplementedError()\n",
        "    for i in range(len(df_split_players_info)):\n",
        "        if type(df_split_players_info[col][i]) == str:\n",
        "            temp = df_split_players_info[col][i][1:-1]\n",
        "            unit = df_split_players_info[col][i][-1]\n",
        "\n",
        "            if temp =='':\n",
        "                temp = '0'\n",
        "            if unit == 'M':\n",
        "                temp = float(temp) * 1000000\n",
        "            elif unit == 'K':\n",
        "                temp = float(temp) * 1000\n",
        "            df_split_players_info[col][i] = float(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "value_and_wage_to_float('Value')\n",
        "value_and_wage_to_float('Wage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6hcwHaZ4znE"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "assert df_split_players_info['Wage'].to_list()[:5] == [230000.0, 350000.0, 420000.0, 450000.0, 195000.0]\n",
        "assert df_split_players_info['Wage'].to_list()[-5:] == [41000.0, 21000.0, 59000.0, 15000.0, 50000.0]\n",
        "assert df_split_players_info['Value'].to_list()[:5] == [190500000.0, 107500000.0, 84000000.0, 64000000.0, 54000000.0]\n",
        "assert df_split_players_info['Value'].to_list()[-5:] == [32000000.0, 9500000.0, 19500000.0, 22000000.0, 20000000.0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HpUsosnEDJia"
      },
      "source": [
        "<b>3)</b> Vì cột giá trị giải phóng hợp đồng (Release Clause) của cầu thủ đang ở dạng object, các bạn cần chuyển về dạng chuỗi và sau đó tiến hành tiền xử lý tương tự như hai cột <b>Value</b> và <b>Wage</b> ở trên bằng cách hoàn thành hàm <b>release_clause_to_float(col)</b>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDExKO517OuE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def release_clause_to_float(col):\n",
        "    # YOUR CODE HERE\n",
        "    # raise NotImplementedError()\n",
        "    for i in range(len(df_split_players_info)):\n",
        "        if type(df_split_players_info[col][i]) == str:\n",
        "            temp = df_split_players_info[col][i][1:-1]\n",
        "            unit = df_split_players_info[col][i][-1]\n",
        "\n",
        "            if temp == '':\n",
        "                temp = '0'\n",
        "            if unit == 'M':\n",
        "                temp = float(temp) * 1000000\n",
        "            elif unit == 'K':\n",
        "                temp = float(temp) * 100000\n",
        "            df_split_players_info[col][i] = float(temp)\n",
        "    df_split_players_info[col] = df_split_players_info[col].fillna(0)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "release_clause_to_float('Release Clause')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkj6nUcEFI8b"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "assert df_split_players_info['Release Clause'].to_list()[:5] == [366700000.0, 198900000.0, 172200000.0, 131199999.99999999, 99900000.0]\n",
        "assert df_split_players_info['Release Clause'].to_list()[-5:] == [0.0, 17100000.0, 38500000.0, 48400000.0, 35500000.0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
